{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from env_wrapper import BoardsWrapper\n",
    "from env_internals import BoardsImplementation\n",
    "from agent_architecture import AgentParams, PPOAgent, RandomAgent, save_agents, load_agents\n",
    "from misc_utils import smooth_list, find_latest_version\n",
    "from training_loop import train_agents, save_stats, load_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game params\n",
    "size = 4\n",
    "n_landmarks = 1\n",
    "n_clues = 1\n",
    "n_questions = 0\n",
    "max_moves = size ** 2 * 4\n",
    "history_len = 4\n",
    "instant_reward_multiplier = 2.0\n",
    "end_reward_multiplier = 10.0\n",
    "\n",
    "# seeds\n",
    "env_seed = 12\n",
    "sender_seed = 135\n",
    "receiver_seed = 246\n",
    "torch_seed = 45954\n",
    "torch.manual_seed(torch_seed)\n",
    "torch.cuda.manual_seed(torch_seed)\n",
    "\n",
    "# training params\n",
    "batch_size = 8\n",
    "n_epochs = 5 # for agents\n",
    "training_epochs = 10\n",
    "n_episodes = 10000\n",
    "gamma = 0.99\n",
    "alpha = 2e-4\n",
    "gae_lambda = 0.95\n",
    "policy_clip = 0.1\n",
    "hidden_size = size ** 2 * 32\n",
    "\n",
    "# other\n",
    "model_version = 0\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions: plot drawing and env init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot drawing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(smoothing_window, all_performances, title, file_path, save=True, show=False):\n",
    "    if not all_performances:\n",
    "        return\n",
    "    smoothed_total = smooth_list(all_performances,smoothing_window)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(smoothed_total)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Smoothed Performance\")\n",
    "    plt.title(title)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(file_path, f\"{title}.png\"), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enviroment init function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env():\n",
    "    env_internals = BoardsImplementation(\n",
    "        size,\n",
    "        n_landmarks,\n",
    "        n_clues,\n",
    "        n_questions,\n",
    "        seed=env_seed\n",
    "    )\n",
    "    env = BoardsWrapper(\n",
    "        env_internals,\n",
    "        max_moves,\n",
    "        history_len,\n",
    "        instant_reward_multiplier,\n",
    "        end_reward_multiplier,\n",
    "        device\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_parameters = AgentParams(gamma,\n",
    "                                alpha,\n",
    "                                gae_lambda,\n",
    "                                policy_clip,\n",
    "                                batch_size,\n",
    "                                n_epochs,\n",
    "                                sender_seed)\n",
    "receiver_parameters = AgentParams(gamma,\n",
    "                                  alpha,\n",
    "                                  gae_lambda,\n",
    "                                  policy_clip,\n",
    "                                  batch_size,\n",
    "                                  n_epochs,\n",
    "                                  receiver_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()\n",
    "sender_agent = PPOAgent(\n",
    "    size,\n",
    "    history_len,\n",
    "    env.sender_n_actions,\n",
    "    hidden_size,\n",
    "    device,\n",
    "    sender_parameters\n",
    ")\n",
    "\n",
    "receiver_agent = PPOAgent(\n",
    "    size,\n",
    "    history_len,\n",
    "    env.receiver_n_actions,\n",
    "    hidden_size,\n",
    "    device,\n",
    "    receiver_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats = {}\n",
    "for i in range(training_epochs):\n",
    "    stats = train_agents(env, sender_agent, receiver_agent, n_episodes)\n",
    "    for key in stats:\n",
    "        if key not in all_stats:\n",
    "            all_stats[key] = []\n",
    "        all_stats[key].extend(stats[key])\n",
    "    # env.render() # create and save animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save params, stats, plots and agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data folder containing stats, plots and agents folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%H%M%S_%Y%m%d\")\n",
    "data_folder = f\"./data_{timestamp}\"\n",
    "os.makedirs(data_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create yaml file with current experiment params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    'game': {\n",
    "        'size': size,\n",
    "        'n_landmarks': n_landmarks,\n",
    "        'n_clues': n_clues,\n",
    "        'n_questions': n_questions,\n",
    "        'max_moves': max_moves,\n",
    "        'history_len': history_len,\n",
    "        'instant_reward_multiplier': instant_reward_multiplier,\n",
    "        'end_reward_multiplier': end_reward_multiplier\n",
    "    },\n",
    "    'seeds': {\n",
    "        'env_seed': env_seed,\n",
    "        'sender_seed': sender_seed,\n",
    "        'receiver_seed': receiver_seed,\n",
    "        'torch_seed': torch_seed\n",
    "    },\n",
    "    'training': {\n",
    "        'batch_size': batch_size,\n",
    "        'n_epochs': n_epochs,\n",
    "        'n_episodes': n_episodes,\n",
    "        'training_epochs': training_epochs,\n",
    "        'gamma': gamma,\n",
    "        'alpha': alpha,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'policy_clip': policy_clip,\n",
    "        'hidden_size': hidden_size\n",
    "    },\n",
    "    'device': device,\n",
    "    'timestamp': datetime.now().strftime(\"%H%M%S_%Y%m%d\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_path = os.path.join(data_folder, f\"experiment_params.yaml\")\n",
    "with open(params_path, 'w') as f:\n",
    "    yaml.dump(experiment_params, f, default_flow_style=False)\n",
    "print(f\"Saved params to {params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder = os.path.join(data_folder, \"models\")\n",
    "os.makedirs(models_folder, exist_ok=True)\n",
    "agents_file_name = f\"agents\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_agents(sender_agent, receiver_agent, os.path.join(models_folder, f\"{agents_file_name}.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stats folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_folder = os.path.join(data_folder, \"stats\")\n",
    "os.makedirs(stats_folder, exist_ok = True)\n",
    "stats_file_name = f\"stats\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_stats(stats, os.path.join(stats_folder, f\"{stats_file_name}.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots folder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_folder = os.path.join(data_folder, \"plots\")\n",
    "os.makedirs(plots_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and save plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "smoothing_window = max(len(all_stats['performances_dist']) // 20, 1)\n",
    "\n",
    "# performance\n",
    "plot_stats(smoothing_window, all_stats['performances_dist'], \"Overall performance\", plots_folder)\n",
    "# final rewards\n",
    "plot_stats(smoothing_window, all_stats['final_rewards_dist'], \"Final rewards\", plots_folder)\n",
    "# episode lengths\n",
    "plot_stats(smoothing_window, all_stats['episode_lengths_dist'], \"Episode lengths\", plots_folder)\n",
    "\n",
    "# avg instant rewards for each actor per episode\n",
    "tmp = [total/length for total, length in zip(all_stats['receiver_instant_rewards_dist'], all_stats['episode_lengths_dist'])]\n",
    "plot_stats(smoothing_window, tmp, \"Avg receiver instant rewards per episode\", plots_folder)\n",
    "tmp = [total/length for total, length in zip(all_stats['sender_instant_rewards_dist'], all_stats['episode_lengths_dist'])]\n",
    "plot_stats(smoothing_window, tmp, \"Avg sender instant rewards per episode\", plots_folder)\n",
    "# avg useless actions for each actor per episode\n",
    "tmp = [total/length for total, length in zip(all_stats['useless_actions_sender_dist'], all_stats['episode_lengths_dist'])]\n",
    "plot_stats(smoothing_window, tmp, \"Avg useless sender actions per episode\", plots_folder)\n",
    "tmp = [total/length for total, length in zip(all_stats['useless_actions_receiver_dist'], all_stats['episode_lengths_dist'])]\n",
    "plot_stats(smoothing_window, tmp, \"Avg useless receiver actions per episode\", plots_folder)\n",
    "# avg empty actions for each actor per episode\n",
    "tmp = [total/length for total, length in zip(all_stats['empty_actions_sender_dist'], all_stats['episode_lengths_dist'])]\n",
    "plot_stats(smoothing_window, tmp, \"Avg empty sender actions per episode\", plots_folder)\n",
    "tmp = [total/length for total, length in zip(all_stats['empty_actions_receiver_dist'], all_stats['episode_lengths_dist'])]\n",
    "plot_stats(smoothing_window, tmp, \"Avg empty receiver actions per episode\", plots_folder)\n",
    "\n",
    "# avg entropy for each actor per episode (already meansâ€”plot directly)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['receiver_entropy_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg receiver entropy per episode\", plots_folder)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['sender_entropy_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg sender entropy per episode\", plots_folder)\n",
    "# avg actor loss for each actor per episode (already means)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['receiver_actor_loss_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg receiver actor loss per episode\", plots_folder)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['sender_actor_loss_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg sender actor loss per episode\", plots_folder)\n",
    "# avg critic loss for each actor per episode (already means)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['receiver_critic_loss_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg receiver critic loss per episode\", plots_folder)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['sender_critic_loss_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg sender critic loss per episode\", plots_folder)\n",
    "# avg total loss for each actor per episode (already means)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['receiver_total_loss_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg receiver total loss per episode\", plots_folder)\n",
    "tmp = [np.mean(ep) if ep else 0 for ep in all_stats['sender_total_loss_dist']]\n",
    "plot_stats(smoothing_window, tmp, \"Avg sender total loss per episode\", plots_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
